{"cells":[{"cell_type":"markdown","metadata":{"id":"ko8K6ntRi-W6"},"source":["# Homework: Word Embedding\n","\n","In this exercise, you will work on the skip-gram neural network architecture for Word2Vec. You will be using Keras to train your model. \n","\n","You must complete the following tasks:\n","1. Read/clean text files\n","2. Indexing (Assign a number to each word)\n","3. Create skip-grams (inputs for your model)\n","4. Create the skip-gram neural network model\n","5. Visualization\n","6. Evaluation (Using pre-trained, not using pre-trained)\n","    (classify topic from 4 categories) \n","    \n","This notebook assumes you have already installed Tensorflow and Keras with python3 and had GPU enabled. If you run this exercise on GCloud using the provided disk image you are all set.\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3351,"status":"ok","timestamp":1677157024488,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"Jw11OhLsi-W8","outputId":"9bc4fab2-9292-432d-e10a-b455baedc0bd"},"outputs":[],"source":["# %tensorflow_version 2.x\n","%matplotlib inline\n","import numpy as np\n","import pandas as pd\n","import math\n","import glob\n","import re\n","import random\n","import collections\n","import os\n","import sys\n","import tensorflow as tf\n","from keras.preprocessing import sequence\n","from keras.models import Sequential, Model\n","from keras.layers import GRU, Dropout\n","from keras.models import load_model\n","from keras.layers import Embedding, Reshape, Activation, Input, Dense, Masking, Conv1D, Bidirectional\n","from tensorflow.python.keras.layers.merge import Dot\n","from tensorflow.python.keras.utils import np_utils\n","from tensorflow.python.keras.utils.data_utils import get_file\n","from tensorflow.python.keras.utils.np_utils import to_categorical\n","from keras.preprocessing.sequence import skipgrams\n","from keras.preprocessing import sequence\n","from keras import backend as K\n","from keras.optimizers import Adam\n","\n","random.seed(42)"]},{"cell_type":"markdown","metadata":{"id":"RdYpL3Uyi-XD"},"source":["# Step 1: Read/clean text files\n","\n","The given code can be used to processed the pre-tokenzied text file from the wikipedia corpus. In your homework, you must replace those text files with raw text files.  You must use your own tokenizer to process your text files"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31077,"status":"ok","timestamp":1677157091968,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"28fKcXgoU-vB","outputId":"81545049-40cb-4376-bbc7-b0bb0aa2b9a9"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":2122,"status":"ok","timestamp":1677157102116,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"Wco1eVRVzn6O","outputId":"f7c716fb-db57-4032-b763-1db27b849945"},"outputs":[],"source":["# import shutil\n","# shutil.copy(\"/content/drive/MyDrive/FRA 501 IntroNLP&DL/Dataset/wiki.zip\",\"/content/wiki.zip\")\n","# shutil.copy(\"/content/drive/MyDrive/FRA 501 IntroNLP&DL/Dataset/BEST-TrainingSet.zip\",\"/content/BEST-TrainingSet.zip\")"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4562,"status":"ok","timestamp":1677157130039,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"BUW1SwAkUIBo","outputId":"eaec628f-abfa-456b-91f8-4d165cc3a403"},"outputs":[],"source":["# !unzip wiki.zip\n","# !unzip BEST-TrainingSet.zip"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"s0ALwvtzDZ-f"},"outputs":[],"source":["#Step 1: read the wikipedia text file\n","with open(\"wiki/thwiki_chk.txt\") as f:\n","    #the delimiter is one or more whitespace characters\n","    input_text = re.compile(r\"\\s+\").split(f.read()) \n","    #exclude an empty string from our input\n","    input_text = [word for word in input_text if word != ''] "]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677157158218,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"KXoFAfjaDcJ2","outputId":"189dc5eb-a62b-41c6-e4e5-1756bf95f40a"},"outputs":[{"name":"stdout","output_type":"stream","text":["['หน้า', 'หลัก', 'วิกิพีเดีย', 'ดำเนินการ', 'โดย', 'มูลนิธิ', 'วิกิ', 'มีเดีย', 'องค์กร', 'ไม่']\n","total word count: 36349066\n"]}],"source":["tokens = input_text\n","print(tokens[:10])\n","print(\"total word count:\", len(tokens))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GDVMvTRci-Xu"},"source":["# Step 2: Indexing (Assign a number to each word)\n","\n","The code below generates an indexed dataset(each word is represented by a number), a dictionary, a reversed dictionary\n","\n","## <font color='purple'>Homework Question 1:</font>\n","<font color='purple'>“UNK” is often used to represent an unknown word (a word which does not exist in your dictionary/training set). You can also represent a rare word with this token as well.  How do you define a rare word in your program? Explain in your own words and capture the screenshot of your code segment that is a part of this process</font>\n","\n"," + <font color='purple'>edit or replace create_index with your own code to set a threshold for rare words and replace them with \"UNK\"</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## <font color='red'>Answer</font>\n","\n","1. Count number of UNK word (if word count <= min_thres_unk, then it is UNK) and at token 'UNK' into word_count dictionary which value is UNK count (need to remove 'UNK' in word_count dictionary because there are word wich spell UNK in input_text).\n","![]()\n","\n","2. Sorted dictionary word_count by using its value.\n","![]()\n","\n","3. Use dicionary to transform word in dataset into sequencs of uniuqe number for each word (for word which has frequency more than min_thres_unk will have their own uniuqe number. for word spell 'UNK' and others use 'UNK' number which is 9 in this dataset)\n","![]()"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18268,"status":"ok","timestamp":1677158042562,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"O6NP7nQGi-Xw","outputId":"80d3e390-d512-49dd-adcc-031a94db7df8"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('ไคเซอร์วิลเฮ็ล์ม', 1), ('Jugen', 1), ('เมืองลอเรนซ์เคิร์ช', 1), ('รัป', 1), ('ค็อปเฟอร์มันน์', 1), ('กับค็อปเฟอร์มันน์', 1), ('เมลท์', 1), ('ลิเซลอตต์', 1), ('(ก.พ.', 1), ('ทักกีสำเร็จการ', 1)]\n","UNK count: 406196\n","pop data: ('UNK', 4)\n","add new data: ('UNK', 406196)\n","Number of sequence in dataset: 36349066\n","Number of word in dictionary: 295164\n"]}],"source":["#step 2:Build dictionary and build a dataset(replace each word with its index)\n","from collections import defaultdict\n","\n","def create_index(input_text, min_thres_unk = 1, max_word_count = None):\n","    # TODO#1 : edit or replace this function\n","    words = [word for word in input_text ]\n","    word_count = list()\n","\n","    #word_count => list of count number of word in each unique words  [(word, count), ...]\n","    #use set and len to get the number of unique words\n","    word_count.extend(collections.Counter(words).most_common(len(set(words))))\n","    #include a token for unknown word\n","    # word_count.append((\"UNK\",0))\n","    #print out 10 most frequent words\n","    print(word_count[-10:])\n","\n","    #thresold to token UNK\n","    count_unk = 0\n","    idx_unk = 0\n","    #loop for counting UNK Word wich frequency equal or less than thresold\n","    for i, pair in enumerate(word_count):\n","        if pair[1] <= min_thres_unk:\n","            count_unk += pair[1]\n","        if pair[0] == \"UNK\": #count \"UNK\" in original input text\n","            count_unk += pair[1]\n","            idx_unk = i\n","\n","    pop_data = word_count.pop(idx_unk) #pop word 'UNK' which in sentence in input text\n","    word_count.append((\"UNK\", count_unk))\n","\n","    print('UNK count:', count_unk)\n","    print('pop data:', pop_data)\n","    print('add new data:', word_count[-1])\n","    \n","    #sort dict word count by using value\n","    word_count = [pair for pair in word_count if pair[1] > min_thres_unk]\n","    word_count = sorted(word_count, key=lambda x: x[1], reverse=True)\n","\n","    #Rank theshold frequency\n","    if max_word_count != None:\n","        word_count = word_count[:max_word_count]\n","\n","    #dictionary => is dict consist of word and unique number for each word {(\"for_keras_zero_padding\", 0), (word1, 1), ...}\n","    dictionary = dict()\n","    dictionary[\"for_keras_zero_padding\"] = 0\n","    for word in word_count:\n","        dictionary[word[0]] = len(dictionary)\n","\n","    #reverse_dictionary is just reverse dictionary : swap values and keys\n","    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n","\n","    #create data set sequences by dictionary\n","    data = list()\n","    for word in input_text:\n","        if word in dictionary.keys():\n","            data.append(dictionary[word])\n","        else: data.append(dictionary['UNK'])\n","\n","    return data, dictionary, reverse_dictionary\n","\n","# call method with min_thres_unk=1ß\n","dataset, dictionary, reverse_dictionary = create_index(tokens, 1)\n","print('Number of sequence in dataset:', len(dataset))\n","print('Number of word in dictionary:', len(dictionary))"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1677158042563,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"2fotaYMgi-Xz","outputId":"065bb577-ce57-40d1-d2c6-cf2b716a9b34"},"outputs":[{"name":"stdout","output_type":"stream","text":["output sample (dataset): [230, 209, 2454, 574, 16, 1830, 7150, 3125, 682, 25]\n","output sample (dictionary): {'for_keras_zero_padding': 0, 'ที่': 1, 'ใน': 2, 'เป็น': 3, 'และ': 4, 'การ': 5, 'มี': 6, 'ของ': 7, 'ได้': 8, 'UNK': 9, ')': 10, '\"': 11, 'จาก': 12}\n","output sample (reverse dictionary): {0: 'for_keras_zero_padding', 1: 'ที่', 2: 'ใน', 3: 'เป็น', 4: 'และ', 5: 'การ', 6: 'มี', 7: 'ของ', 8: 'ได้', 9: 'UNK', 10: ')', 11: '\"', 12: 'จาก'}\n"]}],"source":["print(\"output sample (dataset):\",dataset[:10])\n","print(\"output sample (dictionary):\",{k: dictionary[k] for k in list(dictionary)[:13]})\n","print(\"output sample (reverse dictionary):\",{k: reverse_dictionary[k] for k in list(reverse_dictionary)[:13]})"]},{"cell_type":"markdown","metadata":{"id":"HutTzPO7i-X3"},"source":["# Step3: Create skip-grams (inputs for your model)\n","Keras has a skipgrams-generator, the cell below shows us how it generates skipgrams \n","\n","## <font color='blue'>Homework Question 2:</font>\n","<font color='blue'>The negative samples are sampled from sampling_table.  Look through Keras source code to find out how they sample negative samples. Discuss the sampling technique taught in class and compare it to the Keras source code.</font>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cwYFRO3YGryQ"},"source":["<font color='red'>Q2: PUT YOUR ANWSER HERE!!!</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Negative sample \n","function softmax ใช้การคำนวณมาก"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677158250675,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"C520WnI0i-X4","outputId":"530b4f53-971e-4a00-8058-f9874e5ca017"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[209, 83708], [209, 2454], [2454, 574], [209, 235515], [25, 682], [2454, 209], [3125, 145853], [2454, 115575], [25, 3408], [209, 230], [3125, 682], [3125, 7150], [3125, 285707], [2454, 219950]] [0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n","number of couples: 14\n","หลัก VPNs\n","หลัก วิกิพีเดีย\n","วิกิพีเดีย ดำเนินการ\n","หลัก LaVoie\n","ไม่ องค์กร\n","วิกิพีเดีย หลัก\n","มีเดีย -House\n","วิกิพีเดีย บวรราชสกุล\n"]}],"source":["# Step 3: Create data samples\n","vocab_size = len(dictionary)\n","skip_window = 1       # How many words to consider left and right.\n","\n","# TODO#2 check out keras source code and find out how their sampling technique works. Describe it in your own words.\n","sample_set= dataset[:10]\n","sampling_table = sequence.make_sampling_table(vocab_size)\n","couples, labels = skipgrams(sample_set, vocab_size, window_size=skip_window, sampling_table=sampling_table)\n","word_target, word_context = zip(*couples)\n","word_target = np.array(word_target, dtype=\"int32\")\n","word_context = np.array(word_context, dtype=\"int32\")\n","\n","print(couples, labels)\n","print(\"number of couples:\", len(couples))\n","\n","for i in range(8):\n","    print(reverse_dictionary[couples[i][0]],reverse_dictionary[couples[i][1]])"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["295164"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["vocab_size"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["[230, 209, 2454, 574, 16, 1830, 7150, 3125, 682, 25]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["sample_set"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["array([0.00315225, 0.00315225, 0.00547597, 0.00741556, 0.00912817,\n","       0.01068435, 0.01212381, 0.01347162, 0.01474487, 0.0159558 ,\n","       0.0171136 , 0.01822533, 0.01929662, 0.02033198, 0.02133515,\n","       0.02230924, 0.02325687, 0.02418031, 0.02508148, 0.02596208])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["sampling_table[:20]"]},{"cell_type":"markdown","metadata":{"id":"F6UL0FhEi-X6"},"source":["# Step 4: create the skip-gram model\n","## <font color='blue'>Homework Question 3:</font>\n"," <font color='blue'>Q3:  In your own words, discuss why Sigmoid is chosen as the activation function in the  skip-gram model.</font>"]},{"cell_type":"markdown","metadata":{"id":"-oQLGkkuHG7o"},"source":["<font color='red'>Q3: PUT YOUR ANSER HERE!!!</font>"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5715,"status":"ok","timestamp":1677158262805,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"kq7Eh9pXi-X7","outputId":"f4bd6052-53df-4c40-e81c-562eb2fd5f93"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_2 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," input_1 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, 1, 32)        9445280     ['input_2[0][0]']                \n","                                                                                                  \n"," embedding (Embedding)          (None, 1, 32)        9445280     ['input_1[0][0]']                \n","                                                                                                  \n"," tf.compat.v1.transpose (TFOpLa  (None, 32, 1)       0           ['embedding_1[0][0]']            \n"," mbda)                                                                                            \n","                                                                                                  \n"," tf.linalg.matmul (TFOpLambda)  (None, 1, 1)         0           ['embedding[0][0]',              \n","                                                                  'tf.compat.v1.transpose[0][0]'] \n","                                                                                                  \n"," reshape (Reshape)              (None, 1)            0           ['tf.linalg.matmul[0][0]']       \n","                                                                                                  \n"," activation (Activation)        (None, 1)            0           ['reshape[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 18,890,560\n","Trainable params: 18,890,560\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\envs\\nlp_2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super().__init__(name, **kwargs)\n"]}],"source":["#reference: https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb\n","dim_embedddings = 32\n","V= len(dictionary)\n","\n","#step1: select the embedding of the target word from W\n","w_inputs = Input(shape=(1, ), dtype='int32')\n","w = Embedding(V+1, dim_embedddings)(w_inputs)\n","\n","#step2: select the embedding of the context word from C\n","c_inputs = Input(shape=(1, ), dtype='int32')\n","c  = Embedding(V+1, dim_embedddings)(c_inputs)\n","\n","#step3: compute the dot product:c_k*v_j\n","o = Dot(axes=2)([w, c])\n","o = Reshape((1,), input_shape=(1, 1))(o)\n","\n","#step4: normailize dot products into probability\n","o = Activation('sigmoid')(o)\n","#TO DO#4 Question: Why sigmoid?\n","\n","SkipGram = Model(inputs=[w_inputs, c_inputs], outputs=o)\n","SkipGram.summary()\n","opt=Adam(lr=0.01)\n","SkipGram.compile(loss='binary_crossentropy', optimizer=opt)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"MgR5p_h1i-X9"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6915693879127502 0\n","0.690997302532196 100000\n","0.6901803016662598 200000\n","0.6889075636863708 300000\n","0.6874406933784485 400000\n","0.6856717467308044 500000\n","0.6837462782859802 600000\n","0.681596577167511 700000\n","0.6787207126617432 800000\n","0.6758802533149719 900000\n","0.672584593296051 1000000\n","0.668102502822876 1100000\n","0.6645346283912659 1200000\n","0.6588806509971619 1300000\n","0.653477668762207 1400000\n","0.6492652893066406 1500000\n","0.6420718431472778 1600000\n","0.6341586709022522 1700000\n","0.626152753829956 1800000\n","0.6190935969352722 1900000\n","0.6121807098388672 2000000\n","0.6014773845672607 2100000\n","0.5899662375450134 2200000\n","0.5803203582763672 2300000\n","0.5700560212135315 2400000\n","0.5602562427520752 2500000\n","0.5495749115943909 2600000\n","0.5413700342178345 2700000\n","0.531571626663208 2800000\n","0.5164780020713806 2900000\n","0.5011336803436279 3000000\n","0.48799949884414673 3100000\n","0.4764813780784607 3200000\n","0.46674808859825134 3300000\n","0.45204806327819824 3400000\n","0.43374186754226685 3500000\n","0.419826865196228 3600000\n","0.4061056971549988 3700000\n","0.39151036739349365 3800000\n","0.3770297169685364 3900000\n","0.3622361719608307 4000000\n","0.3468398153781891 4100000\n"]}],"source":["# you don't have to spend too much time training for your homework, you are allowed to do it on a smaller corpus\n","# currently the dataset is 1/20 of the full text file.\n","for _ in range(2):\n","    prev_i=0\n","    #it is likely that your GPU won't be able to handle large input\n","    #just do it 100000 words at a time\n","    for i in range(len(dataset)//100000):\n","        #generate skipgrams\n","        data, labels = skipgrams(sequence=dataset[prev_i*100000:(i*100000)+100000], vocabulary_size=V, window_size=2, negative_samples=4.)\n","        x = [np.array(x) for x in zip(*data)]\n","        y = np.array(labels, dtype=np.int32)\n","        if x:\n","            loss = SkipGram.train_on_batch(x, y)\n","        prev_i = i \n","        print(loss,i*100000)\n","\n","    SkipGram.save_weights('my_skipgram32_weights-hw.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sY69_WFHi-X_"},"outputs":[],"source":["SkipGram.save_weights('my_skipgram32_weights-hw.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7UD13eKki-YA","scrolled":true},"outputs":[],"source":["#Get weight of the embedding layer\n","final_embeddings=SkipGram.get_weights()[0]\n","print(final_embeddings)\n","print(final_embeddings.shape)"]},{"cell_type":"markdown","metadata":{"id":"8ovPmh6Ri-YC"},"source":["# Step 5: Intrinsic Evaluation: Word Vector Analogies\n","## <font color='blue'>Homework Question 4: </font>\n","<font color='blue'> Read section 2.1 and 2.3 in this [lecture note](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf). Come up with 10 semantic analogy examples and report results produced by your word embeddings. Discuss t-SNE in 2 dimensions. </font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8rTxYaLi-YD"},"outputs":[],"source":["# TODO#4:Come up with 10 semantic analogy examples and report results produced by your word embeddings anf do t-SNE in 2 dimensions.\n","#and tell us what you observe "]},{"cell_type":"markdown","metadata":{"id":"sLqG8WaNi-YE"},"source":["# Step 6: Extrinsic Evaluation\n","\n","## <font color='blue'>Homework Question5:</font>\n","<font color='blue'>\n","Use the word embeddings from the skip-gram model as pre-trained weights (GloVe and fastText) in a classification model. Compare the result the with the same classification model that does not use the pre-trained weights. \n","</font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBPutcxEi-YF"},"outputs":[],"source":["all_news_filepath = glob.glob('BEST-TrainingSet/news/*.txt')\n","all_novel_filepath = glob.glob('BEST-TrainingSet/novel/*.txt')\n","all_article_filepath = glob.glob('BEST-TrainingSet/article/*.txt')\n","all_encyclopedia_filepath = glob.glob('BEST-TrainingSet/encyclopedia/*.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZaX-L5n4i-YG"},"outputs":[],"source":["#preparing data for the classificaiton model\n","#In your homework, we will only use the first 2000 words in each text file\n","#any text file that has less than 2000 words will be padded\n","#reason:just to make this homework feasible under limited time and resource\n","import keras\n","import tensorflow\n","\n","max_length = 2000\n","def word_to_index(word):\n","    if word in dictionary:\n","        return dictionary[word]\n","    else:#if unknown\n","        return dictionary[\"UNK\"]\n","\n","\n","def prep_data():\n","    input_text = list()\n","    for textfile_path in [all_news_filepath, all_novel_filepath, all_article_filepath, all_encyclopedia_filepath]:\n","        for input_file in textfile_path:\n","            f = open(input_file,\"r\") #open file with name of \"*.txt\"\n","            text = re.sub(r'\\|', ' ', f.read()) # replace separation symbol with white space           \n","            text = re.sub(r'<\\W?\\w+>', '', text)# remove <NE> </NE> <AB> </AB> tags\n","            text = text.split() #split() method without an argument splits on whitespace \n","            indexed_text = list(map(lambda x:word_to_index(x), text[:max_length])) #map raw word string to its index   \n","            if 'news' in input_file:\n","                input_text.append([indexed_text,0]) \n","            elif 'novel' in input_file:\n","                input_text.append([indexed_text,1]) \n","            elif 'article' in input_file:\n","                input_text.append([indexed_text,2]) \n","            elif 'encyclopedia' in input_file:\n","                input_text.append([indexed_text,3]) \n","            \n","            f.close()\n","    random.shuffle(input_text)\n","    return input_text\n","\n","input_data = prep_data()\n","train_data = input_data[:int(len(input_data)*0.6)]\n","val_data = input_data[int(len(input_data)*0.6):int(len(input_data)*0.8)]\n","test_data = input_data[int(len(input_data)*0.8):]\n","\n","train_input = [data[0] for data in train_data]\n","train_input = keras.utils.pad_sequences(train_input, maxlen=max_length) #padding\n","train_target = [data[1] for data in train_data]\n","train_target=to_categorical(train_target, num_classes=4)\n","\n","val_input = [data[0] for data in val_data]\n","val_input = keras.utils.pad_sequences(val_input, maxlen=max_length) #padding\n","val_target = [data[1] for data in val_data]\n","val_target=to_categorical(val_target, num_classes=4)\n","\n","test_input = [data[0] for data in test_data]\n","test_input = keras.utils.pad_sequences(test_input, maxlen=max_length) #padding\n","test_target = [data[1] for data in test_data]\n","test_target=to_categorical(test_target, num_classes=4)\n","\n","del input_data, val_data,train_data, test_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syrKnUxWi-YI"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_7 (Embedding)     (None, 2000, 32)          9445280   \n","                                                                 \n"," gru_3 (GRU)                 (None, 32)                6336      \n","                                                                 \n"," dropout_3 (Dropout)         (None, 32)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 4)                 132       \n","                                                                 \n","=================================================================\n","Total params: 9,451,748\n","Trainable params: 9,451,748\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train...\n","Epoch 1/10\n","10/10 [==============================] - 22s 2s/step - loss: 1.3454 - accuracy: 0.3663 - val_loss: 1.3315 - val_accuracy: 0.3762\n","Epoch 2/10\n","10/10 [==============================] - 16s 2s/step - loss: 1.1877 - accuracy: 0.4455 - val_loss: 1.1751 - val_accuracy: 0.5050\n","Epoch 3/10\n","10/10 [==============================] - 16s 2s/step - loss: 0.8207 - accuracy: 0.7030 - val_loss: 1.2406 - val_accuracy: 0.5545\n","Epoch 4/10\n","10/10 [==============================] - 16s 2s/step - loss: 0.3515 - accuracy: 0.8911 - val_loss: 1.5419 - val_accuracy: 0.4653\n","Epoch 5/10\n","10/10 [==============================] - 15s 2s/step - loss: 0.2006 - accuracy: 0.9637 - val_loss: 1.6876 - val_accuracy: 0.4950\n","Epoch 6/10\n","10/10 [==============================] - 16s 2s/step - loss: 0.1131 - accuracy: 0.9901 - val_loss: 1.7608 - val_accuracy: 0.5149\n","Epoch 7/10\n","10/10 [==============================] - 16s 2s/step - loss: 0.0502 - accuracy: 0.9967 - val_loss: 1.9657 - val_accuracy: 0.5050\n","Epoch 8/10\n","10/10 [==============================] - 16s 2s/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 2.0897 - val_accuracy: 0.5347\n","Epoch 9/10\n","10/10 [==============================] - 15s 2s/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 2.1355 - val_accuracy: 0.5149\n","Epoch 10/10\n","10/10 [==============================] - 15s 2s/step - loss: 0.0190 - accuracy: 0.9934 - val_loss: 2.1650 - val_accuracy: 0.5743\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x2611c9c5820>"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["#the classification model\n","#TODO#5 find out how to initialize your embedding layer with pre-trained weights, evaluate and observe\n","#don't forget to compare it with the same model that does not use pre-trained weights\n","#you can use your own model too! and feel free to customize this model as you wish\n","# more information --> https://keras.io/examples/nlp/pretrained_word_embeddings/\n","# fastText --> https://fasttext.cc/docs/en/crawl-vectors.html (optional)\n","# !wget --no-check-certificate https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n","\n","cls_model = Sequential()\n","cls_model.add(Embedding(len(dictionary)+1, 32, input_length=max_length,mask_zero=True)) \n","cls_model.add(GRU(32))\n","cls_model.add(Dropout(0.5))\n","cls_model.add(Dense(4, activation='softmax'))\n","opt=Adam(lr=0.01)\n","cls_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","cls_model.summary()\n","print('Train...')\n","cls_model.fit(train_input, train_target,\n","          epochs=10,\n","          validation_data=[val_input, val_target])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4t_dK8l9H92h"},"outputs":[],"source":["results = cls_model.evaluate(test_input, test_target)\n","print(\"test loss, test acc:\", results)"]}],"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
