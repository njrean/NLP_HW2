{"cells":[{"cell_type":"markdown","metadata":{"id":"ko8K6ntRi-W6"},"source":["# Homework: Word Embedding\n","\n","In this exercise, you will work on the skip-gram neural network architecture for Word2Vec. You will be using Keras to train your model. \n","\n","You must complete the following tasks:\n","1. Read/clean text files\n","2. Indexing (Assign a number to each word)\n","3. Create skip-grams (inputs for your model)\n","4. Create the skip-gram neural network model\n","5. Visualization\n","6. Evaluation (Using pre-trained, not using pre-trained)\n","    (classify topic from 4 categories) \n","    \n","This notebook assumes you have already installed Tensorflow and Keras with python3 and had GPU enabled. If you run this exercise on GCloud using the provided disk image you are all set.\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3351,"status":"ok","timestamp":1677157024488,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"Jw11OhLsi-W8","outputId":"9bc4fab2-9292-432d-e10a-b455baedc0bd"},"outputs":[],"source":["# %tensorflow_version 2.x\n","# %matplotlib inline\n","import numpy as np\n","# import pandas as pd\n","import math\n","import glob\n","import re\n","import random\n","import collections\n","import os\n","import sys\n","import tensorflow as tf\n","import keras\n","from keras.preprocessing import sequence\n","from keras.models import Sequential, Model\n","from keras.layers import GRU, Dropout\n","from keras.models import load_model\n","from keras.layers import Embedding, Reshape, Activation, Input, Dense, Masking, Conv1D, Bidirectional\n","from tensorflow.python.keras.layers.merge import Dot\n","from tensorflow.python.keras.utils import np_utils\n","from tensorflow.python.keras.utils.data_utils import get_file\n","from tensorflow.python.keras.utils.np_utils import to_categorical\n","from keras.preprocessing.sequence import skipgrams\n","from keras.preprocessing import sequence\n","from keras import backend as K\n","from keras.optimizers import Adam\n","\n","random.seed(42)"]},{"cell_type":"markdown","metadata":{"id":"RdYpL3Uyi-XD"},"source":["# Step 1: Read/clean text files\n","\n","The given code can be used to processed the pre-tokenzied text file from the wikipedia corpus. In your homework, you must replace those text files with raw text files.  You must use your own tokenizer to process your text files"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31077,"status":"ok","timestamp":1677157091968,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"28fKcXgoU-vB","outputId":"81545049-40cb-4376-bbc7-b0bb0aa2b9a9"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":2122,"status":"ok","timestamp":1677157102116,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"Wco1eVRVzn6O","outputId":"f7c716fb-db57-4032-b763-1db27b849945"},"outputs":[],"source":["# import shutil\n","# shutil.copy(\"/content/drive/MyDrive/FRA 501 IntroNLP&DL/Dataset/wiki.zip\",\"/content/wiki.zip\")\n","# shutil.copy(\"/content/drive/MyDrive/FRA 501 IntroNLP&DL/Dataset/BEST-TrainingSet.zip\",\"/content/BEST-TrainingSet.zip\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4562,"status":"ok","timestamp":1677157130039,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"BUW1SwAkUIBo","outputId":"eaec628f-abfa-456b-91f8-4d165cc3a403"},"outputs":[],"source":["# !unzip wiki.zip\n","# !unzip BEST-TrainingSet.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0ALwvtzDZ-f"},"outputs":[],"source":["#Step 1: read the wikipedia text file\n","with open(\"wiki/thwiki_chk.txt\") as f:\n","    #the delimiter is one or more whitespace characters\n","    input_text = re.compile(r\"\\s+\").split(f.read()) \n","    #exclude an empty string from our input\n","    input_text = [word for word in input_text if word != ''] "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677157158218,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"KXoFAfjaDcJ2","outputId":"189dc5eb-a62b-41c6-e4e5-1756bf95f40a"},"outputs":[{"name":"stdout","output_type":"stream","text":["['หน้า', 'หลัก', 'วิกิพีเดีย', 'ดำเนินการ', 'โดย', 'มูลนิธิ', 'วิกิ', 'มีเดีย', 'องค์กร', 'ไม่']\n","total word count: 36349066\n"]}],"source":["tokens = input_text\n","print(tokens[:10])\n","print(\"total word count:\", len(tokens))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GDVMvTRci-Xu"},"source":["# Step 2: Indexing (Assign a number to each word)\n","\n","The code below generates an indexed dataset(each word is represented by a number), a dictionary, a reversed dictionary\n","\n","## <font color='purple'>Homework Question 1:</font>\n","<font color='purple'>“UNK” is often used to represent an unknown word (a word which does not exist in your dictionary/training set). You can also represent a rare word with this token as well.  How do you define a rare word in your program? Explain in your own words and capture the screenshot of your code segment that is a part of this process</font>\n","\n"," + <font color='purple'>edit or replace create_index with your own code to set a threshold for rare words and replace them with \"UNK\"</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## <font color='red'>Answer</font>\n","\n","1. Count number of UNK word (if word count <= min_thres_unk, then it is UNK) and at token 'UNK' into word_count dictionary which value is UNK count (need to remove 'UNK' in word_count dictionary because there are word which spell UNK in input_text).    \n","\n","![1](https://github.com/njrean/NLP_HW2/blob/main/picture/UNK1.png?raw=true)\n","\n","2. Create dictionary consist of word (key) and index (value) from word in word_count.\n","\n","![2](https://github.com/njrean/NLP_HW2/blob/main/picture/UNK4.png?raw=true)\n","\n","3. Sorted dictionary word_count by using its value.\n","\n","![3](https://github.com/njrean/NLP_HW2/blob/main/picture/UNK2.png?raw=true)\n","\n","4. Use dicionary to transform word in dataset into sequencs of unique number for each word (for word which has frequency more than min_thres_unk will have their own unique number. for word spell 'UNK' and others use 'UNK' number which is 9 in this dataset)\n","\n","![4](https://github.com/njrean/NLP_HW2/blob/main/picture/UNK3.png?raw=true)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18268,"status":"ok","timestamp":1677158042562,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"O6NP7nQGi-Xw","outputId":"80d3e390-d512-49dd-adcc-031a94db7df8"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('ไคเซอร์วิลเฮ็ล์ม', 1), ('Jugen', 1), ('เมืองลอเรนซ์เคิร์ช', 1), ('รัป', 1), ('ค็อปเฟอร์มันน์', 1), ('กับค็อปเฟอร์มันน์', 1), ('เมลท์', 1), ('ลิเซลอตต์', 1), ('(ก.พ.', 1), ('ทักกีสำเร็จการ', 1)]\n","UNK count: 406196\n","pop data: ('UNK', 4)\n","add new data: ('UNK', 406196)\n","Number of sequence in dataset: 36349066\n","Number of word in dictionary: 295164\n"]}],"source":["#step 2:Build dictionary and build a dataset(replace each word with its index)\n","from collections import defaultdict\n","\n","def create_index(input_text, min_thres_unk = 1, max_word_count = None):\n","    # TODO#1 : edit or replace this function\n","    words = [word for word in input_text ]\n","    word_count = list()\n","\n","    #word_count => list of count number of word in each unique words  [(word, count), ...]\n","    #use set and len to get the number of unique words\n","    word_count.extend(collections.Counter(words).most_common(len(set(words))))\n","    #include a token for unknown word\n","    # word_count.append((\"UNK\",0))\n","    #print out 10 most frequent words\n","    print(word_count[-10:])\n","\n","    #thresold to token UNK\n","    count_unk = 0\n","    idx_unk = 0\n","    #loop for counting UNK Word wich frequency equal or less than thresold\n","    for i, pair in enumerate(word_count):\n","        if pair[1] <= min_thres_unk:\n","            count_unk += pair[1]\n","        if pair[0] == \"UNK\": #count \"UNK\" in original input text\n","            count_unk += pair[1]\n","            idx_unk = i\n","\n","    pop_data = word_count.pop(idx_unk) #pop word 'UNK' which in sentence in input text\n","    word_count.append((\"UNK\", count_unk))\n","\n","    print('UNK count:', count_unk)\n","    print('pop data:', pop_data)\n","    print('add new data:', word_count[-1])\n","    \n","    #sort dict word count by using value\n","    word_count = [pair for pair in word_count if pair[1] > min_thres_unk]\n","    word_count = sorted(word_count, key=lambda x: x[1], reverse=True)\n","\n","    #Rank theshold frequency\n","    if max_word_count != None:\n","        word_count = word_count[:max_word_count]\n","\n","    #dictionary => is dict consist of word and unique number for each word {(\"for_keras_zero_padding\", 0), (word1, 1), ...}\n","    dictionary = dict()\n","    dictionary[\"for_keras_zero_padding\"] = 0\n","    for word in word_count:\n","        dictionary[word[0]] = len(dictionary)\n","\n","    #reverse_dictionary is just reverse dictionary : swap values and keys\n","    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n","\n","    #create data set sequences by dictionary\n","    data = list()\n","    for word in input_text:\n","        if word in dictionary.keys():\n","            data.append(dictionary[word])\n","        else: data.append(dictionary['UNK'])\n","\n","    return data, dictionary, reverse_dictionary\n","\n","# call method with min_thres_unk=1ß\n","dataset, dictionary, reverse_dictionary = create_index(tokens, 1)\n","print('Number of sequence in dataset:', len(dataset))\n","print('Number of word in dictionary:', len(dictionary))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1677158042563,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"2fotaYMgi-Xz","outputId":"065bb577-ce57-40d1-d2c6-cf2b716a9b34"},"outputs":[{"name":"stdout","output_type":"stream","text":["output sample (dataset): [230, 209, 2454, 574, 16, 1830, 7150, 3125, 682, 25]\n","output sample (dictionary): {'for_keras_zero_padding': 0, 'ที่': 1, 'ใน': 2, 'เป็น': 3, 'และ': 4, 'การ': 5, 'มี': 6, 'ของ': 7, 'ได้': 8, 'UNK': 9, ')': 10, '\"': 11, 'จาก': 12}\n","output sample (reverse dictionary): {0: 'for_keras_zero_padding', 1: 'ที่', 2: 'ใน', 3: 'เป็น', 4: 'และ', 5: 'การ', 6: 'มี', 7: 'ของ', 8: 'ได้', 9: 'UNK', 10: ')', 11: '\"', 12: 'จาก'}\n"]}],"source":["print(\"output sample (dataset):\",dataset[:10])\n","print(\"output sample (dictionary):\",{k: dictionary[k] for k in list(dictionary)[:13]})\n","print(\"output sample (reverse dictionary):\",{k: reverse_dictionary[k] for k in list(reverse_dictionary)[:13]})"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HutTzPO7i-X3"},"source":["# Step3: Create skip-grams (inputs for your model)\n","Keras has a skipgrams-generator, the cell below shows us how it generates skipgrams \n","\n","## <font color='purple'>Homework Question 2:</font>\n","<font color='purple'>The negative samples are sampled from sampling_table.  Look through Keras source code to find out how they sample negative samples. Discuss the sampling technique taught in class and compare it to the Keras source code.</font>\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cwYFRO3YGryQ"},"source":["<font color='yellow'>Q2: PUT YOUR ANWSER HERE!!!</font>\n","## <font color='red'>Answer</font>\n","\n","From vanilla skip-gram model has serveral disavantages such as it use high computational cost from soft max activation function. So, one of alternative ways to solve it is skip-gram negative sampling\n","\n","Negative sample is the pair (terget word, context word) which context word is not in skip_window neighbor of target word\n","\n","The original idea of skip-gram model is to prodict probability to decide which word is the context word of input word (target word) like illustrated in Fig1. And you will see that the compution of each proablity for each word for comparing the true lable is expensive see in equation below. Therefore, The idea of negative sampling is converting multi-classification task into binary-classification task. It changes to predict for a given input pair of target word and context word, whether the given context word is in window of target word or not.\n","\n","<center><img src=\"https://github.com/njrean/NLP_HW2/blob/main/picture/skipgram1.png?raw=true\" width=\"500\"></center>\n","\n","<center><img src=\"https://github.com/njrean/NLP_HW2/blob/main/picture/skipgram2.png?raw=true\" width=\"300\"></center>\n","\n","According to negative skip-gram idea as mention before, you will see we pair of target word and context word to be a input and label positive (1) or negative (0) samples to ba an output so we need to prepare dataset like this.\n","\n","For keras source code in this section, is the way to provide dataset to train negative sampling skip-graam model, generating pair of target word and context word. In code, start by generate sampling table to use in skipgrams function it will return proability of sampling for each word. Probability of common word will less than rare word because we do not need balance random word. Skipgrams function generate random pair of target word and context word which are both positive samples and negative samples. Then, return variable couples which is pair of sample consist of positive samples and negative smaple as the same number and variable label which is list of lable positive sample (1) or negative sample (0) of each pair samples.\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677158250675,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"C520WnI0i-X4","outputId":"530b4f53-971e-4a00-8058-f9874e5ca017"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[209, 83708], [209, 2454], [2454, 574], [209, 235515], [25, 682], [2454, 209], [3125, 145853], [2454, 115575], [25, 3408], [209, 230], [3125, 682], [3125, 7150], [3125, 285707], [2454, 219950]] [0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n","number of couples: 14\n","หลัก VPNs\n","หลัก วิกิพีเดีย\n","วิกิพีเดีย ดำเนินการ\n","หลัก LaVoie\n","ไม่ องค์กร\n","วิกิพีเดีย หลัก\n","มีเดีย -House\n","วิกิพีเดีย บวรราชสกุล\n"]}],"source":["# Step 3: Create data samples\n","vocab_size = len(dictionary)\n","skip_window = 1       # How many words to consider left and right.\n","\n","# TODO#2 check out keras source code and find out how their sampling technique works. Describe it in your own words.\n","sample_set= dataset[:10]\n","sampling_table = sequence.make_sampling_table(vocab_size)\n","couples, labels = sequence.skipgrams(sample_set, vocab_size, window_size=skip_window, sampling_table=sampling_table, seed=27)\n","word_target, word_context = zip(*couples)\n","word_target = np.array(word_target, dtype=\"int32\")\n","word_context = np.array(word_context, dtype=\"int32\")\n","\n","print(couples, labels)\n","print(\"number of couples:\", len(couples))\n","\n","for i in range(8):\n","    print(reverse_dictionary[couples[i][0]],reverse_dictionary[couples[i][1]])"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["295164"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["vocab_size"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["[230, 209, 2454, 574, 16, 1830, 7150, 3125, 682, 25]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["sample_set"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["array([0.00315225, 0.00315225, 0.00547597, 0.00741556, 0.00912817,\n","       0.01068435, 0.01212381, 0.01347162, 0.01474487, 0.0159558 ,\n","       0.0171136 , 0.01822533, 0.01929662, 0.02033198, 0.02133515,\n","       0.02230924, 0.02325687, 0.02418031, 0.02508148, 0.02596208])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["sampling_table[:20]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"F6UL0FhEi-X6"},"source":["# Step 4: create the skip-gram model\n","## <font color='purple'>Homework Question 3:</font>\n"," <font color='purple'>Q3:  In your own words, discuss why Sigmoid is chosen as the activation function in the  skip-gram model.</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-oQLGkkuHG7o"},"source":["<font color='yellow'>Q3: PUT YOUR ANSER HERE!!!</font>\n","## <font color='red'>Answer</font>\n","\n","According to the idea of the negative skipgram we mention in Question 2, the reasons why sigmoid is chosen in this skip-gram model are the negative skip-gram mode is precit binary classification problem. The prediction is the the context word in pair is context in window neighbor of the target or not. "]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5715,"status":"ok","timestamp":1677158262805,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"kq7Eh9pXi-X7","outputId":"f4bd6052-53df-4c40-e81c-562eb2fd5f93"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_2 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," input_1 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, 1, 32)        9445280     ['input_2[0][0]']                \n","                                                                                                  \n"," embedding (Embedding)          (None, 1, 32)        9445280     ['input_1[0][0]']                \n","                                                                                                  \n"," tf.compat.v1.transpose (TFOpLa  (None, 32, 1)       0           ['embedding_1[0][0]']            \n"," mbda)                                                                                            \n","                                                                                                  \n"," tf.linalg.matmul (TFOpLambda)  (None, 1, 1)         0           ['embedding[0][0]',              \n","                                                                  'tf.compat.v1.transpose[0][0]'] \n","                                                                                                  \n"," reshape (Reshape)              (None, 1)            0           ['tf.linalg.matmul[0][0]']       \n","                                                                                                  \n"," activation (Activation)        (None, 1)            0           ['reshape[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 18,890,560\n","Trainable params: 18,890,560\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Anaconda3\\envs\\nlp_2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super().__init__(name, **kwargs)\n"]}],"source":["#reference: https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb\n","dim_embedddings = 32\n","V= len(dictionary)\n","\n","#step1: select the embedding of the target word from W\n","w_inputs = Input(shape=(1, ), dtype='int32')\n","w = Embedding(V+1, dim_embedddings)(w_inputs)\n","\n","#step2: select the embedding of the context word from C\n","c_inputs = Input(shape=(1, ), dtype='int32')\n","c  = Embedding(V+1, dim_embedddings)(c_inputs)\n","\n","#step3: compute the dot product:c_k*v_j\n","o = Dot(axes=2)([w, c])\n","o = Reshape((1,), input_shape=(1, 1))(o)\n","\n","#step4: normailize dot products into probability\n","o = Activation('sigmoid')(o)\n","#TO DO#4 Question: Why sigmoid?\n","\n","SkipGram = Model(inputs=[w_inputs, c_inputs], outputs=o)\n","SkipGram.summary()\n","opt=Adam(lr=0.01)\n","SkipGram.compile(loss='binary_crossentropy', optimizer=opt)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"MgR5p_h1i-X9"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6915693879127502 0\n","0.690997302532196 100000\n","0.6901803016662598 200000\n","0.6889075636863708 300000\n","0.6874406933784485 400000\n","0.6856717467308044 500000\n","0.6837462782859802 600000\n","0.681596577167511 700000\n","0.6787207126617432 800000\n","0.6758802533149719 900000\n","0.672584593296051 1000000\n","0.668102502822876 1100000\n","0.6645346283912659 1200000\n","0.6588806509971619 1300000\n","0.653477668762207 1400000\n","0.6492652893066406 1500000\n","0.6420718431472778 1600000\n","0.6341586709022522 1700000\n","0.626152753829956 1800000\n","0.6190935969352722 1900000\n","0.6121807098388672 2000000\n","0.6014773845672607 2100000\n","0.5899662375450134 2200000\n","0.5803203582763672 2300000\n","0.5700560212135315 2400000\n","0.5602562427520752 2500000\n","0.5495749115943909 2600000\n","0.5413700342178345 2700000\n","0.531571626663208 2800000\n","0.5164780020713806 2900000\n","0.5011336803436279 3000000\n","0.48799949884414673 3100000\n","0.4764813780784607 3200000\n","0.46674808859825134 3300000\n","0.45204806327819824 3400000\n","0.43374186754226685 3500000\n","0.419826865196228 3600000\n","0.4061056971549988 3700000\n","0.39151036739349365 3800000\n","0.3770297169685364 3900000\n","0.3622361719608307 4000000\n","0.3468398153781891 4100000\n","0.3389534056186676 4200000\n","0.33950984477996826 4300000\n","0.32136040925979614 4400000\n","0.29826804995536804 4500000\n","0.29541704058647156 4600000\n","0.2846078872680664 4700000\n","0.26897189021110535 4800000\n","0.2593592405319214 4900000\n","0.24921074509620667 5000000\n","0.2450224608182907 5100000\n","0.24559862911701202 5200000\n","0.23528076708316803 5300000\n","0.2252998799085617 5400000\n","0.2194707840681076 5500000\n","0.21909433603286743 5600000\n","0.21763044595718384 5700000\n","0.20667608082294464 5800000\n","0.19891265034675598 5900000\n","0.19751381874084473 6000000\n","0.1957954466342926 6100000\n","0.18970578908920288 6200000\n","0.1898748129606247 6300000\n","0.18838217854499817 6400000\n","0.17947867512702942 6500000\n","0.17995105683803558 6600000\n","0.18379056453704834 6700000\n","0.17875058948993683 6800000\n","0.17569410800933838 6900000\n","0.1732623130083084 7000000\n","0.17012540996074677 7100000\n","0.1732504814863205 7200000\n","0.17616140842437744 7300000\n","0.17178112268447876 7400000\n","0.1662839651107788 7500000\n","0.16896142065525055 7600000\n","0.17053040862083435 7700000\n","0.1640346497297287 7800000\n","0.15803176164627075 7900000\n","0.16037015616893768 8000000\n","0.1638801246881485 8100000\n","0.1605895459651947 8200000\n","0.15955662727355957 8300000\n","0.16519694030284882 8400000\n","0.1640624850988388 8500000\n","0.16069835424423218 8600000\n","0.1598242074251175 8700000\n","0.16047589480876923 8800000\n","0.16347303986549377 8900000\n","0.16010861098766327 9000000\n","0.1541461944580078 9100000\n","0.1545962244272232 9200000\n","0.15777340531349182 9300000\n","0.15641862154006958 9400000\n","0.15941190719604492 9500000\n","0.15886127948760986 9600000\n","0.1507716178894043 9700000\n","0.15464738011360168 9800000\n","0.15116149187088013 9900000\n","0.14585894346237183 10000000\n","0.15467409789562225 10100000\n","0.15906323492527008 10200000\n","0.1621125340461731 10300000\n","0.16051942110061646 10400000\n","0.15275801718235016 10500000\n","0.15751168131828308 10600000\n","0.16095252335071564 10700000\n","0.1511145383119583 10800000\n","0.14961911737918854 10900000\n","0.15203455090522766 11000000\n","0.14544761180877686 11100000\n","0.14346842467784882 11200000\n","0.14508947730064392 11300000\n","0.14584127068519592 11400000\n","0.14368905127048492 11500000\n","0.14140939712524414 11600000\n","0.14073307812213898 11700000\n","0.14386910200119019 11800000\n","0.15167289972305298 11900000\n","0.15040361881256104 12000000\n","0.14608429372310638 12100000\n","0.14341896772384644 12200000\n","0.14196191728115082 12300000\n","0.1475619524717331 12400000\n","0.1488489955663681 12500000\n","0.14477580785751343 12600000\n","0.14234057068824768 12700000\n","0.1460486799478531 12800000\n","0.15015125274658203 12900000\n","0.15055404603481293 13000000\n","0.1484672725200653 13100000\n","0.14367736876010895 13200000\n","0.1427851915359497 13300000\n","0.147730752825737 13400000\n","0.15366600453853607 13500000\n","0.15079139173030853 13600000\n","0.1508215069770813 13700000\n","0.15431645512580872 13800000\n","0.15176783502101898 13900000\n","0.14716781675815582 14000000\n","0.14444224536418915 14100000\n","0.1453157365322113 14200000\n","0.14994943141937256 14300000\n","0.15043368935585022 14400000\n","0.14423871040344238 14500000\n","0.14732453227043152 14600000\n","0.14738377928733826 14700000\n","0.14082752168178558 14800000\n","0.14025147259235382 14900000\n","0.13952146470546722 15000000\n","0.14568066596984863 15100000\n","0.15053041279315948 15200000\n","0.148265078663826 15300000\n","0.1431661993265152 15400000\n","0.13958501815795898 15500000\n","0.13859358429908752 15600000\n","0.1397687941789627 15700000\n","0.14197038114070892 15800000\n","0.14367008209228516 15900000\n","0.14992405474185944 16000000\n","0.1502750962972641 16100000\n","0.14098940789699554 16200000\n","0.13528282940387726 16300000\n","0.1452694982290268 16400000\n","0.14666464924812317 16500000\n","0.1396295428276062 16600000\n","0.14071476459503174 16700000\n","0.13748891651630402 16800000\n","0.1301237940788269 16900000\n","0.13386571407318115 17000000\n","0.1391594409942627 17100000\n","0.13959163427352905 17200000\n","0.14146679639816284 17300000\n","0.14331494271755219 17400000\n","0.14810597896575928 17500000\n","0.1493930220603943 17600000\n","0.14297626912593842 17700000\n","0.13776127994060516 17800000\n","0.14119066298007965 17900000\n","0.14025026559829712 18000000\n","0.13870784640312195 18100000\n","0.1359325796365738 18200000\n","0.12953828275203705 18300000\n","0.13407130539417267 18400000\n","0.13688822090625763 18500000\n","0.13325273990631104 18600000\n","0.13534337282180786 18700000\n","0.1346278339624405 18800000\n","0.13303136825561523 18900000\n","0.1347055584192276 19000000\n","0.14040011167526245 19100000\n","0.14048166573047638 19200000\n","0.13555370271205902 19300000\n","0.13522981107234955 19400000\n","0.1328652948141098 19500000\n","0.13151244819164276 19600000\n","0.1338415890932083 19700000\n","0.13166092336177826 19800000\n","0.13606049120426178 19900000\n","0.1388610154390335 20000000\n","0.1347493976354599 20100000\n","0.13936488330364227 20200000\n","0.1379823386669159 20300000\n","0.13617126643657684 20400000\n","0.13681676983833313 20500000\n","0.13497605919837952 20600000\n","0.13728615641593933 20700000\n","0.13803404569625854 20800000\n","0.13392998278141022 20900000\n","0.1317625194787979 21000000\n","0.12574048340320587 21100000\n","0.123715341091156 21200000\n","0.12936842441558838 21300000\n","0.12718217074871063 21400000\n","0.12604308128356934 21500000\n","0.13043047487735748 21600000\n","0.12937599420547485 21700000\n","0.1270378679037094 21800000\n","0.1292305290699005 21900000\n","0.11980331689119339 22000000\n","0.11544407159090042 22100000\n","0.12863288819789886 22200000\n","0.13687925040721893 22300000\n","0.13292758166790009 22400000\n","0.12955830991268158 22500000\n","0.12753607332706451 22600000\n","0.12381953746080399 22700000\n","0.12478034943342209 22800000\n","0.12941080331802368 22900000\n","0.13465756177902222 23000000\n","0.13273952901363373 23100000\n","0.1284189075231552 23200000\n","0.12947626411914825 23300000\n","0.12693916261196136 23400000\n","0.12604418396949768 23500000\n","0.13061247766017914 23600000\n","0.12515117228031158 23700000\n","0.1218748390674591 23800000\n","0.12407034635543823 23900000\n","0.12769094109535217 24000000\n","0.12475995719432831 24100000\n","0.1234588548541069 24200000\n","0.12413490563631058 24300000\n","0.122011199593544 24400000\n","0.11929704993963242 24500000\n","0.1185997948050499 24600000\n","0.12269942462444305 24700000\n","0.1270904242992401 24800000\n","0.128708615899086 24900000\n","0.1251114457845688 25000000\n","0.12401189655065536 25100000\n","0.1249304935336113 25200000\n","0.12493260949850082 25300000\n","0.1254902184009552 25400000\n","0.1257322132587433 25500000\n","0.12610718607902527 25600000\n","0.12367219477891922 25700000\n","0.12109921127557755 25800000\n","0.12354068458080292 25900000\n","0.12349456548690796 26000000\n","0.1251724362373352 26100000\n","0.13026650249958038 26200000\n","0.13215196132659912 26300000\n","0.13215680420398712 26400000\n","0.130605548620224 26500000\n","0.12901075184345245 26600000\n","0.12588223814964294 26700000\n","0.12594708800315857 26800000\n","0.12499786913394928 26900000\n","0.121174655854702 27000000\n","0.12303166836500168 27100000\n","0.12544633448123932 27200000\n","0.12103600054979324 27300000\n","0.12098877876996994 27400000\n","0.12828633189201355 27500000\n","0.12229148298501968 27600000\n","0.12030939757823944 27700000\n","0.12779831886291504 27800000\n","0.13056093454360962 27900000\n","0.12813113629817963 28000000\n","0.12354772537946701 28100000\n","0.12282439321279526 28200000\n","0.12047572433948517 28300000\n","0.119999960064888 28400000\n","0.12225515395402908 28500000\n","0.12187276780605316 28600000\n","0.12349914014339447 28700000\n","0.12873880565166473 28800000\n","0.1342724710702896 28900000\n","0.13104960322380066 29000000\n","0.1250223070383072 29100000\n","0.12843942642211914 29200000\n","0.13085508346557617 29300000\n","0.1309037208557129 29400000\n","0.12904000282287598 29500000\n","0.12480567395687103 29600000\n","0.1308003067970276 29700000\n","0.13615617156028748 29800000\n","0.1304939091205597 29900000\n","0.13210158050060272 30000000\n","0.13323675096035004 30100000\n","0.1286260187625885 30200000\n","0.1304033100605011 30300000\n","0.1328566074371338 30400000\n","0.1267472207546234 30500000\n","0.12156025320291519 30600000\n","0.12390029430389404 30700000\n","0.12665900588035583 30800000\n","0.12761390209197998 30900000\n","0.12687014043331146 31000000\n","0.12455111742019653 31100000\n","0.1245199665427208 31200000\n","0.12977831065654755 31300000\n","0.12717658281326294 31400000\n","0.12540943920612335 31500000\n","0.13257503509521484 31600000\n","0.13216237723827362 31700000\n","0.13177581131458282 31800000\n","0.12814831733703613 31900000\n","0.12408016622066498 32000000\n","0.12285859137773514 32100000\n","0.1278940588235855 32200000\n","0.1312962770462036 32300000\n","0.12402274459600449 32400000\n","0.12604694068431854 32500000\n","0.12818372249603271 32600000\n","0.12142098695039749 32700000\n","0.12404325604438782 32800000\n","0.1247170940041542 32900000\n","0.11940311640501022 33000000\n","0.11834219843149185 33100000\n","0.11770566552877426 33200000\n","0.1217508539557457 33300000\n","0.12460097670555115 33400000\n","0.1242125853896141 33500000\n","0.11921422928571701 33600000\n","0.11434923112392426 33700000\n","0.12001814693212509 33800000\n","0.12606899440288544 33900000\n","0.12595175206661224 34000000\n","0.12692219018936157 34100000\n","0.12735901772975922 34200000\n","0.1292247176170349 34300000\n","0.12938623130321503 34400000\n","0.12513409554958344 34500000\n","0.11680501699447632 34600000\n","0.11707070469856262 34700000\n","0.11871214210987091 34800000\n","0.11554355174303055 34900000\n","0.1160079762339592 35000000\n","0.11570233106613159 35100000\n","0.11606495827436447 35200000\n","0.11814871430397034 35300000\n","0.12093758583068848 35400000\n","0.1279170662164688 35500000\n","0.13240262866020203 35600000\n","0.12088803201913834 35700000\n","0.11400608718395233 35800000\n","0.1180061548948288 35900000\n","0.11757640540599823 36000000\n","0.11416503041982651 36100000\n","0.118451327085495 36200000\n","0.10314731299877167 0\n","0.10656903684139252 100000\n","0.10651160776615143 200000\n","0.09981886297464371 300000\n","0.10073305666446686 400000\n","0.10403483361005783 500000\n","0.0989171713590622 600000\n","0.09920339286327362 700000\n","0.10334653407335281 800000\n","0.11008235067129135 900000\n","0.1105225682258606 1000000\n","0.10278147459030151 1100000\n","0.10658296942710876 1200000\n","0.10326587408781052 1300000\n","0.10543477535247803 1400000\n","0.11358287185430527 1500000\n","0.11208025366067886 1600000\n","0.11026269942522049 1700000\n","0.10881669074296951 1800000\n","0.10865373909473419 1900000\n","0.11105930060148239 2000000\n","0.10641872137784958 2100000\n","0.10301760584115982 2200000\n","0.10502053797245026 2300000\n","0.10565581917762756 2400000\n","0.10921680182218552 2500000\n","0.10808747261762619 2600000\n","0.11517459154129028 2700000\n","0.12410707771778107 2800000\n","0.12058494240045547 2900000\n","0.11418185383081436 3000000\n","0.11408945918083191 3100000\n","0.12115460634231567 3200000\n","0.12410102784633636 3300000\n","0.12314695119857788 3400000\n","0.12022732198238373 3500000\n","0.1179792582988739 3600000\n","0.11513139307498932 3700000\n","0.112178735435009 3800000\n","0.11224198341369629 3900000\n","0.11337466537952423 4000000\n","0.11309707909822464 4100000\n","0.1186283603310585 4200000\n","0.12646231055259705 4300000\n","0.11793429404497147 4400000\n","0.10944012552499771 4500000\n","0.11991263926029205 4600000\n","0.1195039302110672 4700000\n","0.1115955039858818 4800000\n","0.1124095544219017 4900000\n","0.11197766661643982 5000000\n","0.11536984145641327 5100000\n","0.11970604211091995 5200000\n","0.11551149934530258 5300000\n","0.11327719688415527 5400000\n","0.11365467309951782 5500000\n","0.11677948385477066 5600000\n","0.1196095272898674 5700000\n","0.11495315283536911 5800000\n","0.1111549437046051 5900000\n","0.112664133310318 6000000\n","0.11489921063184738 6100000\n","0.11275311559438705 6200000\n","0.11368154734373093 6300000\n","0.11467340588569641 6400000\n","0.11013243347406387 6500000\n","0.11189330369234085 6600000\n","0.1164412647485733 6700000\n","0.11334513127803802 6800000\n","0.11231629550457001 6900000\n","0.11224737018346786 7000000\n","0.11056891083717346 7100000\n","0.11447738856077194 7200000\n","0.11771965026855469 7300000\n","0.1155768483877182 7400000\n","0.11292093247175217 7500000\n","0.11562120169401169 7600000\n","0.11726947128772736 7700000\n","0.11342702805995941 7800000\n","0.10930309444665909 7900000\n","0.11131300032138824 8000000\n","0.1159808561205864 8100000\n","0.11431591212749481 8200000\n","0.11365124583244324 8300000\n","0.11550212651491165 8400000\n","0.11560830473899841 8500000\n","0.1156773641705513 8600000\n","0.11439356207847595 8700000\n","0.11535534262657166 8800000\n","0.11789007484912872 8900000\n","0.11600890755653381 9000000\n","0.11165483295917511 9100000\n","0.11234857887029648 9200000\n","0.1155378669500351 9300000\n","0.11509314179420471 9400000\n","0.11799401044845581 9500000\n","0.11814375966787338 9600000\n","0.11190363764762878 9700000\n","0.11409446597099304 9800000\n","0.11262787133455276 9900000\n","0.10961250215768814 10000000\n","0.11632222682237625 10100000\n","0.12005169689655304 10200000\n","0.12271776050329208 10300000\n","0.12029395252466202 10400000\n","0.11528050154447556 10500000\n","0.12043622881174088 10600000\n","0.12282243371009827 10700000\n","0.11597853899002075 10800000\n","0.1148943305015564 10900000\n","0.11736498773097992 11000000\n","0.11343587934970856 11100000\n","0.11156269162893295 11200000\n","0.1124759241938591 11300000\n","0.11281271278858185 11400000\n","0.11171948164701462 11500000\n","0.11038967967033386 11600000\n","0.10994262248277664 11700000\n","0.11229240894317627 11800000\n","0.11821524053812027 11900000\n","0.11741865426301956 12000000\n","0.11456922441720963 12100000\n","0.11314929276704788 12200000\n","0.11198589950799942 12300000\n","0.1158987507224083 12400000\n","0.11709138005971909 12500000\n","0.11459167301654816 12600000\n","0.11282305419445038 12700000\n","0.11539552360773087 12800000\n","0.11834080517292023 12900000\n","0.11841628700494766 13000000\n","0.11765772104263306 13100000\n","0.11429114639759064 13200000\n","0.1136290580034256 13300000\n","0.1166539341211319 13400000\n","0.12035468220710754 13500000\n","0.1188073381781578 13600000\n","0.11931829154491425 13700000\n","0.12299711257219315 13800000\n","0.12126747518777847 13900000\n","0.11695553362369537 14000000\n","0.1141607016324997 14100000\n","0.11555919051170349 14200000\n","0.12001395970582962 14300000\n","0.12058033049106598 14400000\n","0.11535871028900146 14500000\n","0.11480430513620377 14600000\n","0.11628556251525879 14700000\n","0.1137048751115799 14800000\n","0.11286143958568573 14900000\n","0.11219820380210876 15000000\n","0.11726821959018707 15100000\n","0.12128806859254837 15200000\n","0.11943609267473221 15300000\n","0.1166897714138031 15400000\n","0.11427661031484604 15500000\n","0.11383886635303497 15600000\n","0.11437768489122391 15700000\n","0.11599266529083252 15800000\n","0.11741796135902405 15900000\n","0.12059041857719421 16000000\n","0.11944648623466492 16100000\n","0.11416653543710709 16200000\n","0.11106859147548676 16300000\n","0.11609907448291779 16400000\n","0.11753341555595398 16500000\n","0.11505500227212906 16600000\n","0.1156383827328682 16700000\n","0.11311626434326172 16800000\n","0.10726770013570786 16900000\n","0.1107914000749588 17000000\n","0.11478827148675919 17100000\n","0.11473672837018967 17200000\n","0.11495355516672134 17300000\n","0.11685691028833389 17400000\n","0.12082235515117645 17500000\n","0.12150882184505463 17600000\n","0.11783923208713531 17700000\n","0.113319993019104 17800000\n","0.11588039994239807 17900000\n","0.11547969281673431 18000000\n","0.11435829848051071 18100000\n","0.11264857649803162 18200000\n","0.1075206771492958 18300000\n","0.1107892096042633 18400000\n","0.11325456202030182 18500000\n","0.1108962744474411 18600000\n","0.1122966781258583 18700000\n","0.11216304451227188 18800000\n","0.11078399419784546 18900000\n","0.11253882199525833 19000000\n","0.11717267334461212 19100000\n","0.11685384064912796 19200000\n","0.11313460767269135 19300000\n","0.11271656304597855 19400000\n","0.11102219671010971 19500000\n","0.10982923209667206 19600000\n","0.11187371611595154 19700000\n","0.1105201244354248 19800000\n","0.11383380740880966 19900000\n","0.11558889597654343 20000000\n","0.11238764226436615 20100000\n","0.11639764159917831 20200000\n","0.11513032019138336 20300000\n","0.11401274055242538 20400000\n","0.11478129029273987 20500000\n","0.1132107526063919 20600000\n","0.11555536836385727 20700000\n","0.11576677858829498 20800000\n","0.1126064732670784 20900000\n","0.11043709516525269 21000000\n","0.10567720979452133 21100000\n","0.10432116687297821 21200000\n","0.10818774998188019 21300000\n","0.10694349557161331 21400000\n","0.10552924871444702 21500000\n","0.10898268967866898 21600000\n","0.1083841398358345 21700000\n","0.1064147800207138 21800000\n","0.10843697190284729 21900000\n","0.10012194514274597 22000000\n","0.0963384360074997 22100000\n","0.10767940431833267 22200000\n","0.1150028258562088 22300000\n","0.11235714703798294 22400000\n","0.10991562902927399 22500000\n","0.10802315920591354 22600000\n","0.1048555001616478 22700000\n","0.10578188300132751 22800000\n","0.10944455862045288 22900000\n","0.11334583908319473 23000000\n","0.11207964271306992 23100000\n","0.10930712521076202 23200000\n","0.1109113022685051 23300000\n","0.10871566087007523 23400000\n","0.10818566381931305 23500000\n","0.11180280894041061 23600000\n","0.10754192620515823 23700000\n","0.10415022075176239 23800000\n","0.10606413334608078 23900000\n","0.10913068801164627 24000000\n","0.10689292103052139 24100000\n","0.10595835000276566 24200000\n","0.10637330263853073 24300000\n","0.1048026978969574 24400000\n","0.10270680487155914 24500000\n","0.10216369479894638 24600000\n","0.1049598678946495 24700000\n","0.10840463638305664 24800000\n","0.10991288721561432 24900000\n","0.10728594660758972 25000000\n","0.106983482837677 25100000\n","0.10701814293861389 25200000\n","0.10722415894269943 25300000\n","0.10770196467638016 25400000\n","0.10785463452339172 25500000\n","0.10736292600631714 25600000\n","0.10656958818435669 25700000\n","0.10471799969673157 25800000\n","0.1066850945353508 25900000\n","0.1062362864613533 26000000\n","0.10713217407464981 26100000\n","0.11209559440612793 26200000\n","0.11262398958206177 26300000\n","0.11252591013908386 26400000\n","0.11224370449781418 26500000\n","0.11082250624895096 26600000\n","0.1081191822886467 26700000\n","0.10812866687774658 26800000\n","0.10799936950206757 26900000\n","0.10429567843675613 27000000\n","0.10578929632902145 27100000\n","0.10821878165006638 27200000\n","0.10457265377044678 27300000\n","0.10448309034109116 27400000\n","0.10964315384626389 27500000\n","0.10499439388513565 27600000\n","0.10353152453899384 27700000\n","0.10980306565761566 27800000\n","0.11198980361223221 27900000\n","0.11003457009792328 28000000\n","0.10664895921945572 28100000\n","0.10623107105493546 28200000\n","0.10431884974241257 28300000\n","0.10422036796808243 28400000\n","0.10544774681329727 28500000\n","0.10559200495481491 28600000\n","0.10695937275886536 28700000\n","0.11038995534181595 28800000\n","0.11528138816356659 28900000\n","0.11310123652219772 29000000\n","0.10788676142692566 29100000\n","0.11057615280151367 29200000\n","0.11198506504297256 29300000\n","0.11229417473077774 29400000\n","0.11090333014726639 29500000\n","0.10770048946142197 29600000\n","0.11238638311624527 29700000\n","0.11659032106399536 29800000\n","0.1122191771864891 29900000\n","0.11389024555683136 30000000\n","0.11460229009389877 30100000\n","0.11074299365282059 30200000\n","0.11247126758098602 30300000\n","0.11452548205852509 30400000\n","0.10971130430698395 30500000\n","0.10509119182825089 30600000\n","0.10770529508590698 30700000\n","0.11033602803945541 30800000\n","0.11070284247398376 30900000\n","0.11009912192821503 31000000\n","0.10752351582050323 31100000\n","0.10750145465135574 31200000\n","0.11131709814071655 31300000\n","0.10955959558486938 31400000\n","0.10839278995990753 31500000\n","0.11372910439968109 31600000\n","0.11370186507701874 31700000\n","0.1138293668627739 31800000\n","0.11139273643493652 31900000\n","0.10829303413629532 32000000\n","0.10683540999889374 32100000\n","0.1107824295759201 32200000\n","0.11392196267843246 32300000\n","0.10760119557380676 32400000\n","0.1092732697725296 32500000\n","0.11065004765987396 32600000\n","0.10458304733037949 32700000\n","0.10710830241441727 32800000\n","0.10834933072328568 32900000\n","0.10395229607820511 33000000\n","0.10321962833404541 33100000\n","0.10306917876005173 33200000\n","0.10523885488510132 33300000\n","0.10747329145669937 33400000\n","0.10752031952142715 33500000\n","0.10330699384212494 33600000\n","0.09914697706699371 33700000\n","0.10422036796808243 33800000\n","0.10801706463098526 33900000\n","0.10848038643598557 34000000\n","0.1098671406507492 34100000\n","0.11032698303461075 34200000\n","0.11155598610639572 34300000\n","0.11178237199783325 34400000\n","0.10829830914735794 34500000\n","0.10125433653593063 34600000\n","0.10092437267303467 34700000\n","0.10232900828123093 34800000\n","0.10083861649036407 34900000\n","0.10156355798244476 35000000\n","0.10198275744915009 35100000\n","0.10151268541812897 35200000\n","0.10290149599313736 35300000\n","0.103797048330307 35400000\n","0.10922548174858093 35500000\n","0.11429814994335175 35600000\n","0.10433169454336166 35700000\n","0.09853439778089523 35800000\n","0.10267731547355652 35900000\n","0.10265180468559265 36000000\n","0.09949450194835663 36100000\n","0.10447132587432861 36200000\n"]}],"source":["# you don't have to spend too much time training for your homework, you are allowed to do it on a smaller corpus\n","# currently the dataset is 1/20 of the full text file.\n","for _ in range(2):\n","    prev_i=0\n","    #it is likely that your GPU won't be able to handle large input\n","    #just do it 100000 words at a time\n","    for i in range(len(dataset)//100000):\n","        #generate skipgrams\n","        data, labels = skipgrams(sequence=dataset[prev_i*100000:(i*100000)+100000], vocabulary_size=V, window_size=2, negative_samples=4.)\n","        x = [np.array(x) for x in zip(*data)]\n","        y = np.array(labels, dtype=np.int32)\n","        if x:\n","            loss = SkipGram.train_on_batch(x, y)\n","        prev_i = i \n","        print(loss,i*100000)\n","\n","    SkipGram.save_weights('my_skipgram32_weights-hw.h5')\n"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"sY69_WFHi-X_"},"outputs":[],"source":["SkipGram.save_weights('my_skipgram32_weights-hw.h5')"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"7UD13eKki-YA","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.02942849 -0.01651738  0.00090514 ... -0.0396731  -0.00961649\n","   0.0410727 ]\n"," [ 0.58344823  0.6269893  -0.6127344  ... -0.5630088   0.60312194\n","  -0.55613977]\n"," [ 0.621968    0.5677286  -0.5895863  ... -0.60078937  0.60932505\n","  -0.58522063]\n"," ...\n"," [-0.41199094 -0.47773108  0.46366584 ...  0.49456057 -0.4612527\n","   0.45464048]\n"," [-0.4276229  -0.48767427  0.41189465 ...  0.422792   -0.4493855\n","   0.45252112]\n"," [-0.04287151 -0.01097244  0.00668874 ... -0.00614158  0.02317256\n","  -0.03015853]]\n","(295165, 32)\n"]}],"source":["#Get weight of the embedding layer\n","final_embeddings=SkipGram.get_weights()[0]\n","print(final_embeddings)\n","print(final_embeddings.shape)"]},{"cell_type":"markdown","metadata":{"id":"8ovPmh6Ri-YC"},"source":["# Step 5: Intrinsic Evaluation: Word Vector Analogies\n","## <font color='blue'>Homework Question 4: </font>\n","<font color='blue'> Read section 2.1 and 2.3 in this [lecture note](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf). Come up with 10 semantic analogy examples and report results produced by your word embeddings. Discuss t-SNE in 2 dimensions. </font>\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"k8rTxYaLi-YD"},"outputs":[],"source":["# TODO#4:Come up with 10 semantic analogy examples and report results produced by your word embeddings anf do t-SNE in 2 dimensions.\n","#and tell us what you observe "]},{"cell_type":"markdown","metadata":{"id":"sLqG8WaNi-YE"},"source":["# Step 6: Extrinsic Evaluation\n","\n","## <font color='blue'>Homework Question5:</font>\n","<font color='blue'>\n","Use the word embeddings from the skip-gram model as pre-trained weights (GloVe and fastText) in a classification model. Compare the result the with the same classification model that does not use the pre-trained weights. \n","</font>\n"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"dBPutcxEi-YF"},"outputs":[],"source":["all_news_filepath = glob.glob('BEST-TrainingSet/news/*.txt')\n","all_novel_filepath = glob.glob('BEST-TrainingSet/novel/*.txt')\n","all_article_filepath = glob.glob('BEST-TrainingSet/article/*.txt')\n","all_encyclopedia_filepath = glob.glob('BEST-TrainingSet/encyclopedia/*.txt')"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"ZaX-L5n4i-YG"},"outputs":[],"source":["#preparing data for the classificaiton model\n","#In your homework, we will only use the first 2000 words in each text file\n","#any text file that has less than 2000 words will be padded\n","#reason:just to make this homework feasible under limited time and resource\n","import keras\n","import tensorflow\n","\n","max_length = 2000\n","def word_to_index(word):\n","    if word in dictionary:\n","        return dictionary[word]\n","    else:#if unknown\n","        return dictionary[\"UNK\"]\n","\n","\n","def prep_data():\n","    input_text = list()\n","    for textfile_path in [all_news_filepath, all_novel_filepath, all_article_filepath, all_encyclopedia_filepath]:\n","        for input_file in textfile_path:\n","            f = open(input_file,\"r\") #open file with name of \"*.txt\"\n","            text = re.sub(r'\\|', ' ', f.read()) # replace separation symbol with white space           \n","            text = re.sub(r'<\\W?\\w+>', '', text)# remove <NE> </NE> <AB> </AB> tags\n","            text = text.split() #split() method without an argument splits on whitespace \n","            indexed_text = list(map(lambda x:word_to_index(x), text[:max_length])) #map raw word string to its index   \n","            if 'news' in input_file:\n","                input_text.append([indexed_text,0]) \n","            elif 'novel' in input_file:\n","                input_text.append([indexed_text,1]) \n","            elif 'article' in input_file:\n","                input_text.append([indexed_text,2]) \n","            elif 'encyclopedia' in input_file:\n","                input_text.append([indexed_text,3]) \n","            \n","            f.close()\n","    random.shuffle(input_text)\n","    return input_text\n","\n","input_data = prep_data()\n","train_data = input_data[:int(len(input_data)*0.6)]\n","val_data = input_data[int(len(input_data)*0.6):int(len(input_data)*0.8)]\n","test_data = input_data[int(len(input_data)*0.8):]\n","\n","train_input = [data[0] for data in train_data]\n","train_input = keras.utils.pad_sequences(train_input, maxlen=max_length) #padding\n","train_target = [data[1] for data in train_data]\n","train_target=to_categorical(train_target, num_classes=4)\n","\n","val_input = [data[0] for data in val_data]\n","val_input = keras.utils.pad_sequences(val_input, maxlen=max_length) #padding\n","val_target = [data[1] for data in val_data]\n","val_target=to_categorical(val_target, num_classes=4)\n","\n","test_input = [data[0] for data in test_data]\n","test_input = keras.utils.pad_sequences(test_input, maxlen=max_length) #padding\n","test_target = [data[1] for data in test_data]\n","test_target=to_categorical(test_target, num_classes=4)\n","\n","del input_data, val_data,train_data, test_data"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"syrKnUxWi-YI"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 2000, 32)          9445280   \n","                                                                 \n"," gru (GRU)                   (None, 32)                6336      \n","                                                                 \n"," dropout (Dropout)           (None, 32)                0         \n","                                                                 \n"," dense (Dense)               (None, 4)                 132       \n","                                                                 \n","=================================================================\n","Total params: 9,451,748\n","Trainable params: 9,451,748\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train...\n","Epoch 1/10\n","10/10 [==============================] - 13s 1s/step - loss: 1.3564 - accuracy: 0.3597 - val_loss: 1.3207 - val_accuracy: 0.4158\n","Epoch 2/10\n","10/10 [==============================] - 9s 874ms/step - loss: 1.1998 - accuracy: 0.5050 - val_loss: 1.2346 - val_accuracy: 0.5050\n","Epoch 3/10\n","10/10 [==============================] - 9s 868ms/step - loss: 0.6837 - accuracy: 0.7888 - val_loss: 1.1449 - val_accuracy: 0.5050\n","Epoch 4/10\n","10/10 [==============================] - 9s 874ms/step - loss: 0.2339 - accuracy: 0.9439 - val_loss: 2.1658 - val_accuracy: 0.3960\n","Epoch 5/10\n","10/10 [==============================] - 9s 878ms/step - loss: 0.0781 - accuracy: 0.9802 - val_loss: 2.1393 - val_accuracy: 0.5347\n","Epoch 6/10\n","10/10 [==============================] - 9s 889ms/step - loss: 0.0504 - accuracy: 0.9835 - val_loss: 2.1377 - val_accuracy: 0.5050\n","Epoch 7/10\n","10/10 [==============================] - 9s 898ms/step - loss: 0.0555 - accuracy: 0.9934 - val_loss: 2.3052 - val_accuracy: 0.4653\n","Epoch 8/10\n","10/10 [==============================] - 9s 898ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 2.5528 - val_accuracy: 0.4752\n","Epoch 9/10\n","10/10 [==============================] - 9s 908ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 2.6915 - val_accuracy: 0.4950\n","Epoch 10/10\n","10/10 [==============================] - 9s 901ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 2.7580 - val_accuracy: 0.5248\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x28a457a3d90>"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["#the classification model\n","#TODO#5 find out how to initialize your embedding layer with pre-trained weights, evaluate and observe\n","#don't forget to compare it with the same model that does not use pre-trained weights\n","#you can use your own model too! and feel free to customize this model as you wish\n","# more information --> https://keras.io/examples/nlp/pretrained_word_embeddings/\n","# fastText --> https://fasttext.cc/docs/en/crawl-vectors.html (optional)\n","# !wget --no-check-certificate https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n","\n","cls_model = Sequential()\n","cls_model.add(Embedding(len(dictionary)+1, 32, input_length=max_length,mask_zero=True)) \n","cls_model.add(GRU(32))\n","cls_model.add(Dropout(0.5))\n","cls_model.add(Dense(4, activation='softmax'))\n","opt=Adam(lr=0.01)\n","cls_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","cls_model.summary()\n","print('Train...')\n","cls_model.fit(train_input, train_target,\n","          epochs=10,\n","          validation_data=[val_input, val_target])"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"4t_dK8l9H92h"},"outputs":[{"name":"stdout","output_type":"stream","text":["4/4 [==============================] - 1s 178ms/step - loss: 2.4912 - accuracy: 0.5392\n","test loss, test acc: [2.4911601543426514, 0.5392156839370728]\n"]}],"source":["results = cls_model.evaluate(test_input, test_target)\n","print(\"test loss, test acc:\", results)"]}],"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
